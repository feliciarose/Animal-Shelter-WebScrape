import requests
from bs4 import BeautifulSoup
from lxml import html
import pandas as pd

#Main function
#Must have content etc
#Must loop through each page to pull all of the info 

URL = 'https://public-solutions.hillsboroughcounty.org/enterprise/f?p=236:11:::::P11_PET_TYPE:Dog'
page = requests.get(URL)
#print the entire page: print(page.text)

#easier reading and finding 
soup = BeautifulSoup(page.content, "html.parser")
#prints the entire page: print(soup)

#Line 139 contains links to every pet profile 

path = 'https://public-solutions.hillsboroughcounty.org/enterprise/'
#full path = partial path + href
#full path: https://public-solutions.hillsboroughcounty.org/enterprise/f?p=ADOPT:PET_DETAILS:::::P20_ANIMAL_ID_2,P20_TYPE,P20_FROM:A2251954,DOG,2
'''
 ><td class=" u-tL" headers="C409348400771462906"><a href="f?p=ADOPT:PET_DETAILS:::::P20_ANIMAL_ID_2,P20_TYPE,P20_FROM:A2251954,DOG,2" 
 data-animal_id="A2251954"><img class="dog" src="https://public-solutions.hillsboroughcounty.org/enterprise/pet_prod.adopt_image_src?p_id=A2251954&which=DOG" 
 " style="width:160px;" alt="Daeva" title="Daeva" ></a></td><td class=" u-tL" headers="C409338796119462898">Pre-Adoptable 
 <span class="click-help a-Icon icon-help" aria-hidden="true" id="Pre-Adoptable"></span></td><td class=" u-tL" headers="C409346019706462904">
 <a href="f?p=ADOPT:PET_DETAILS:::::P20_ANIMAL_ID_2,P20_TYPE,P20_FROM:A2251954,DOG,2"><u>Daeva</u></a></td><td class=" u-tL" 
 headers="C409344028609462903">A2251954</td><td class=" u-tL" รง
 headers="C409346438289462905">Water Main @ Pleasant Mobile Home Park On 301 S</td><td class=" u-tL" 
 headers="C234427330483331921">991001911287997</td><td class=" u-tR" headers="C6711368978400503">312</td></tr><tr 
'''

tree = html.fromstring(page.text)

#We NEED to pull names here
#Pull Details, but not name (but ID number, kennel number, breed, microchip, location found)
details = tree.xpath('//td[contains(@class, " u-tL")]')
#make this not a list later, this is not efficent
#use numpy or dataframe!!!!!
unfiltered_info = []
for detail in details:
    unfiltered_info.append(detail.text)

info = [detail for detail in unfiltered_info if detail != None]
#print(info)

#Need to Pull: Kennel #(2) (multiplies of 2), Gender(3) (multiplies of 3),  Status(0) (multiples of 7)
x, y, z = 2, 3, 0
kennel, gender, status = [], [], []
while x <= len(info):
    kennel.append(info[x])
    gender.append(info[y])
    status.append(info[z])
    x+=7
    y+=7
    z+=7

'''
print(kennel)
print(gender)
print(status)
print(len(kennel))
print(len(gender))
print(len(status))
'''

#Lotia
#Pulled this way because it is in ><
lotia = []
lotias = tree.xpath('//td[contains(@class, " u-tR")]')
for day in lotias:
    lotia.append(day.text)

#Gets every individual name
names = []
for link in soup.findAll('img'):
    names.append(link.get('alt'))

#remove logo names (first and last are not dogs)
names.remove(names[0])
names.remove(names[len(names)-1])

#Gets every individual link to connect to profile
#Gets animal ID
petLinks = []
unfiltered_animalID = []
for link in soup.findAll('a'):
    href = link.get('href')
    id = link.get('data-animal_id')
    petLinks.append(href)
    unfiltered_animalID.append(id)

    if href.startswith('f'):
        continue
    else:
        petLinks.remove(href) 

animalID = [id for id in unfiltered_animalID if id != None]

#Remove first and last link it is not relevant
petLinks.remove(petLinks[0])
petLinks.remove(petLinks[len(petLinks)-1])

#Remove duplicate paths
#High time complexity fix later if this script is running too slow
links = []
for id in petLinks:
    if id not in links:
        links.append(id)

#Gets full paths
paths = []
for link in links:
    paths.append(path+link)

print(paths)

#Organize Data in dictionary
data = {}
#'Kennel', 'Name', 'Gender', 'ID', 'Status', 'Lotia'
data['Kennel'] = kennel
data['Name'] = names
data['ID'] = animalID
data['Gender'] = gender
data['Status'] = status
data['Lotia'] = lotia
data['Path'] = paths

'''
print(len(kennel))
print(len(names))
print(len(gender))
print(len(status))
print(len(lotia))
print(len(animalID))
print(len(paths))
'''

df = pd.DataFrame(data)
print(df)



